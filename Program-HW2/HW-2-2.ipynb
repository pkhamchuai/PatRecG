{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 2\n",
    "\n",
    "Due Date 21 กันยายน 2566 ก่อนเวลา 23.00 น.\n",
    "\n",
    "ให้ส่งเป็น pdf file ผ่านระบบที่ mango.cmu.ac.th เท่านั้น\n",
    "\n",
    "Note ห้าม ใช้ library หรือ โปรแกรมสำหรับรูปใดๆ ทั้งสิ้น จะต้องเขียนโปรแกรมด้วยตัวเองทั้งหมด ห้าม ลอกงานคนอื่นถ้าลอกกันมา ทั้งคนให้ลอกและคนลอกจะได้รับคะแนนเป็น 0 ทั้งคู่ และให้แนบ program มาในส่วนของภาคผนวกของรายงานด้วย\n",
    "\n",
    "การบ้านนี้มี 2 ข้อ\n",
    "\n",
    "## 2.2 จงทำการทดลองโดยใช้ string grammar Hard C-means \n",
    "โดยใช้ data set ของ Copenhagen Chromosome Database ให้ใช้ train data set ในการ train (ชื่อ file จะเป็น difxda เช่น dif1da) และ test data set  (ชื่อ file จะเป็น difxdb เช่น dif1db) ในการ test ข้อมูลของ data set อยู่ที่ http://myweb.cmu.ac.th/sansanee.a/PatRecG/dataset/chrom.zip ทั้งนี้ data set มีจำนวน chromosome ทั้งหมด 2200 chromosome อย่าลืมอ่านคำแนะนำ data set ใน file CopenhagenChromosomeDataset.html ด้วย\n",
    "\n",
    " \n",
    "\n",
    "รายงานควรจะประกอบด้วย\n",
    "\n",
    "            1. รายละเอียดของทฤษฎีหรือวิธีการต่างๆที่ใช้\n",
    "\n",
    "            2. การออกแบบ algorithm เช่น pseudo-code, flowchart, ฯลฯ\n",
    "\n",
    "            3. ผลการทดลอง\n",
    "\n",
    "            4. การวิเคราะห์การทดลอง เช่น ได้ผลตามที่คาดไว้หรือไม่ มีสิ่งประหลาดเกิดขึ้นหรือไม่ บทสรุปที่ได้คืออะไร ฯลฯ\n",
    "\n",
    "            5. Well documented, structured, modular program listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files: 44\n",
      "# of training files: 22\n",
      "# of testing files:  22\n"
     ]
    }
   ],
   "source": [
    "# grab the name of files in the directory 'chrom'\n",
    "# excluding the file with 'html' extension\n",
    "# and store the names in a list\n",
    "\n",
    "chrom_files = glob.glob('chrom/*')\n",
    "chrom_files = [x for x in chrom_files if 'html' not in x]\n",
    "print(f'# of files: {len(chrom_files)}')\n",
    "\n",
    "# files ending with a are training data\n",
    "# files ending with b are testing data\n",
    "\n",
    "# list of training data\n",
    "train_files = [x for x in chrom_files if 'a' in x]\n",
    "print(f'# of training files: {len(train_files)}')\n",
    "\n",
    "# list of testing data\n",
    "test_files = [x for x in chrom_files if 'b' in x]\n",
    "print(f'# of testing files:  {len(test_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that reads the contents of a file\n",
    "# and returns a list of lines\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # for each line, take only the string after '\\t' and before '\\n'\n",
    "    lines = [x.split('\\t')[1].split('\\n')[0] for x in lines]\n",
    "\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data: 2200\n",
      "# of testing data: 2200\n"
     ]
    }
   ],
   "source": [
    "# read all the training data\n",
    "training_data = [read_file(x) for x in train_files]\n",
    "# flatten the list of lists\n",
    "training_data = [item for sublist in training_data for item in sublist]\n",
    "print(f'# of training data: {len(training_data)}')\n",
    "\n",
    "# read all the testing data\n",
    "test_data = [read_file(x) for x in test_files]\n",
    "# flatten the list of lists\n",
    "test_data = [item for sublist in test_data for item in sublist]\n",
    "print(f'# of testing data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String grammar hard C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Levenshtein distance function\n",
    "def levenshtein_distance(s1, s2):\n",
    "    # if one of the strings is empty, return the length of the other string\n",
    "    if len(s1) == 0:\n",
    "        return len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    # if the last characters of the strings are the same,\n",
    "    # return the distance between the strings without the last character\n",
    "    if s1[-1] == s2[-1]:\n",
    "        return levenshtein_distance(s1[:-1], s2[:-1])\n",
    "\n",
    "    # otherwise, return the minimum of the following:\n",
    "    # 1. distance between s1 and s2 without the last character of s1\n",
    "    # 2. distance between s1 and s2 without the last character of s2\n",
    "    # 3. distance between s1 and s2 without the last character of both s1 and s2\n",
    "    return 1 + min(levenshtein_distance(s1[:-1], s2),\n",
    "                   levenshtein_distance(s1, s2[:-1]),\n",
    "                   levenshtein_distance(s1[:-1], s2[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between 'kitten' and 'sitting' is 3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "distance = levenshtein_distance(string1, string2)\n",
    "print(f\"Levenshtein distance between '{string1}' and '{string2}' is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    # Create a matrix to store the distances between substrings of s1 and s2\n",
    "    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    for i in range(len(s1) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill in the matrix using dynamic programming\n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,  # Deletion\n",
    "                dp[i][j - 1] + 1,  # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    # The final value in the bottom-right corner of the matrix is the Levenshtein distance\n",
    "    return dp[len(s1)][len(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between 'kitten' and 'sitting' is 3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "distance = levenshtein_distance(string1, string2)\n",
    "print(f\"Levenshtein distance between '{string1}' and '{string2}' is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Training data: 220\n",
      "Initial centroids: ['A=====C===a===A==a=====A==b====D==d======D===d===A==b==Aa=a'\n",
      " 'A==B==a===D==b==A=a==A==b===A=a===C===a=A=a==A==c====A==a=A===c=A=a=a'\n",
      " 'A==D==aA==a==A=aA=a=B===bAb===A=a==A===c==Aa==a']\n",
      "Iteration 1\n",
      "Assigning clusters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Number of train samples in each cluster: None\n",
      "Shape of u_ik: (220, 3)\n",
      "Sum of u_ik: [ 76.  30. 114.]\n",
      "Old centroids: ['A=====C===a===A==a=====A==b====D==d======D===d===A==b==Aa=a'\n",
      " 'A==B==a===D==b==A=a==A==b===A=a===C===a=A=a==A==c====A==a=A===c=A=a=a'\n",
      " 'A==D==aA==a==A=aA=a=B===bAb===A=a==A===c==Aa==a']\n",
      "Cluster 0: 0/76\n",
      "Samples: [1, 3, 5, 6, 11, 12, 13, 16, 20, 21, 26, 28, 30, 31, 32, 34, 36, 40, 44, 45, 47, 50, 51, 52, 53, 61, 63, 65, 68, 69, 74, 79, 81, 93, 96, 97, 98, 100, 101, 106, 107, 108, 109, 111, 115, 116, 117, 120, 121, 124, 125, 126, 130, 134, 139, 144, 150, 154, 156, 166, 167, 170, 171, 172, 179, 184, 187, 189, 190, 194, 198, 209, 212, 217, 219]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m         \u001b[39mprint\u001b[39m(predictions)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m         \u001b[39m################################# end #################################\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m cross_validation(training_data, test_data, fold\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=199'>200</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining data: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data_train_fold)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=200'>201</a>\u001b[0m classifier \u001b[39m=\u001b[39m sgHCM(k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(data_train_fold)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m predictions \u001b[39m=\u001b[39m [classifier(test_sample) \u001b[39mfor\u001b[39;00m test_sample \u001b[39min\u001b[39;00m test_data]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "\u001b[1;32m/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_centroids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcentroids\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOld centroids: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_centroids\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_centroids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_centroids(train_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNew centroids: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_centroids\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# compute the difference between old and new centroids\u001b[39;00m\n",
      "\u001b[1;32m/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSamples: \u001b[39m\u001b[39m{\u001b[39;00msamples\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# take train_data at the index of samples\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m other_samples \u001b[39m=\u001b[39m train_data[samples]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOther samples: \u001b[39m\u001b[39m{\u001b[39;00mother_samples\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pkhamchuai/codes/PatRecG/Program-HW2/HW-2-2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m distances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_distance(train_data[j], train_data[samples])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "class sgHCM:\n",
    "    # first, initial the class with k value\n",
    "    # then, call the class with test sample and train data\n",
    "    def __init__(self, k=3, max_iter=1000, tol=1e-5):\n",
    "        # number of clusters\n",
    "        self.k = k\n",
    "        # maximum iteration\n",
    "        self.max_iter = max_iter\n",
    "        # tolerance (epsilon)\n",
    "        self.tol = tol\n",
    "        # initialize the list of distances between old and new centroids very high\n",
    "        self.Et = np.inf\n",
    "\n",
    "    def __call__(self, test_sample):\n",
    "        # compute the distance between test_sample and each train data\n",
    "        self.distances = self.compute_distance(test_sample, train_data)\n",
    "\n",
    "        # find the index of the closest centroid\n",
    "        closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "        # assign the test sample to the closest centroid\n",
    "        self.predicted_label = closest_centroid\n",
    "\n",
    "        return self.predicted_label\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        # randomly select k centroids from train_data (or prototype vectors)\n",
    "        self.centroids = np.random.choice(train_data, size=self.k, replace=False)\n",
    "        print(f'Initial centroids: {self.centroids}')\n",
    "\n",
    "        # initialize the list of clusters\n",
    "        self.clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # initialize the list of old centroids\n",
    "        self.old_centroids = None\n",
    "\n",
    "        # initialize the list of new centroids\n",
    "        self.new_centroids = None\n",
    "\n",
    "        # initialize the list of distances between test sample and each train data\n",
    "        self.distances = None\n",
    "\n",
    "        # initialize the list of predicted labels\n",
    "        self.predicted_label = None\n",
    "\n",
    "        self.u_ik = None\n",
    "        self.n_it = None\n",
    "\n",
    "        for t in range(self.max_iter):\n",
    "            print(f'Iteration {t + 1}')\n",
    "            # assign each train data to the closest centroid\n",
    "            self.clusters, self.u_ik = self.assign_clusters(train_data)\n",
    "\n",
    "            # number of train samples in each cluster\n",
    "            print(f'Number of train samples in each cluster: {self.n_it}')\n",
    "            # print shape of u_ik\n",
    "            print(f'Shape of u_ik: {self.u_ik.shape}')\n",
    "            print(f'Sum of u_ik: {np.sum(self.u_ik, axis=0)}')\n",
    "\n",
    "            # n_it is the number of train samples in the i-th cluster\n",
    "            self.n_it = np.sum(self.u_ik, axis=0)\n",
    "\n",
    "            # update the centroids\n",
    "            self.old_centroids = self.centroids\n",
    "            print(f'Old centroids: {self.old_centroids}')\n",
    "            self.new_centroids = self.update_centroids(train_data)\n",
    "            print(f'New centroids: {self.new_centroids}')\n",
    "\n",
    "            # compute the difference between old and new centroids\n",
    "            self.Et = self.compute_terminal_measure(self.old_centroids, self.new_centroids)\n",
    "\n",
    "            # if the difference is less than the tolerance, stop the iteration\n",
    "            if self.Et < self.tol:\n",
    "                break\n",
    "\n",
    "            # otherwise, update the centroids and continue\n",
    "            self.centroids = self.new_centroids\n",
    "\n",
    "        print(f'Final centroids: {self.centroids}')\n",
    "\n",
    "    def assign_clusters(self, train_data):\n",
    "        # compute the distance between the train data and each centroid\n",
    "        print(f'Assigning clusters...')\n",
    "        # initialize the list of clusters\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # for each train data\n",
    "        for i, train_sample in enumerate(train_data):\n",
    "            # compute the distance between test_sample and the centroid\n",
    "            self.distances = self.compute_distance(train_sample, self.centroids)\n",
    "\n",
    "            # find the index of the closest centroid\n",
    "            closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "            # assign the train sample to the closest centroid\n",
    "            clusters[closest_centroid].append(i)\n",
    "\n",
    "        # create the u_ik matrix, which is a matrix of 0s and 1s\n",
    "        # u_ik[i][j] = 1 if the i-th train sample belongs to the j-th cluster\n",
    "        # u_ik[i][j] = 0 otherwise\n",
    "        u_ik = np.zeros((len(train_data), self.k))\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            for j in cluster:\n",
    "                u_ik[j][i] = 1\n",
    "\n",
    "        print(f'Done.')\n",
    "        return clusters, u_ik\n",
    "    \n",
    "    def update_centroids(self, train_data):\n",
    "        # compute the distance between each sample and other samples in the same cluster\n",
    "        # the sample with the minimum sum of the distances to other samples in the same cluster is the new centroid for that cluster  \n",
    "\n",
    "        # initialize the list of new centroids\n",
    "        new_centroids = []\n",
    "\n",
    "        # for each cluster\n",
    "        for i in range(len(self.clusters)):\n",
    "            # c_ij is a list of the sum of the distances to other samples in the same cluster\n",
    "            c_ij = []\n",
    "\n",
    "            for j in range(len(self.clusters[i])):\n",
    "                # show progress\n",
    "                if j % 20 == 0:\n",
    "                    print(f'Cluster {i}: {j}/{len(self.clusters[i])}')\n",
    "\n",
    "                # compute the distance between the sample and other samples in the same cluster excluding itself\n",
    "                # list of samples excluding the sample itself\n",
    "                samples = [x for x in self.clusters[i] if x != j]\n",
    "                print(f'Samples: {samples}')\n",
    "                # take train_data at the index of samples\n",
    "                other_samples = train_data[samples]\n",
    "                print(f'Other samples: {other_samples}')\n",
    "\n",
    "                distances = self.compute_distance(train_data[j], train_data[samples])\n",
    "                \n",
    "                c_ij.append(np.sum(distances/self.n_it[i], axis=0))\n",
    "\n",
    "            # find the index of the sample with the minimum sum \n",
    "            # of the distances to other samples in the same cluster\n",
    "            alpha_q = np.argmin(c_ij)\n",
    "            print(f'New centroid for cluster {i}: {train_data[alpha_q]}')\n",
    "\n",
    "            # add the new centroid to the list\n",
    "            new_centroids.append(train_data[alpha_q])\n",
    "\n",
    "        return new_centroids\n",
    "    \n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        # Create a matrix to store the distances between substrings of s1 and s2\n",
    "        dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "        # Initialize the first row and column of the matrix\n",
    "        for i in range(len(s1) + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(len(s2) + 1):\n",
    "            dp[0][j] = j\n",
    "\n",
    "        # Fill in the matrix using dynamic programming\n",
    "        for i in range(1, len(s1) + 1):\n",
    "            for j in range(1, len(s2) + 1):\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,  # Deletion\n",
    "                    dp[i][j - 1] + 1,  # Insertion\n",
    "                    dp[i - 1][j - 1] + cost  # Substitution\n",
    "                )\n",
    "\n",
    "        # The final value in the bottom-right corner of the matrix is the Levenshtein distance\n",
    "        return dp[len(s1)][len(s2)]\n",
    "\n",
    "    def compute_distance(self, test_sample, train_data):\n",
    "        # compute the distance between test_sample and each train data\n",
    "        # return a list of distances\n",
    "        distances = []\n",
    "        for train_sample in train_data:\n",
    "            distance = self.levenshtein_distance(test_sample, train_sample)\n",
    "            distances.append(distance)\n",
    "        return distances\n",
    "    \n",
    "    def compute_terminal_measure(self, old, new):\n",
    "        # compute the difference between old and new centroids\n",
    "        Et = np.sum(np.abs(old - new))\n",
    "\n",
    "        # return the tolerance\n",
    "        return Et\n",
    "    \n",
    "# perform 10-fold cross validation on sgHCM classifier\n",
    "def cross_validation(train_data, test_data, fold=10):\n",
    "    # shuffle data before cross validation\n",
    "    np.random.shuffle(train_data)\n",
    "    fold_size = len(train_data) // fold\n",
    "\n",
    "    for i in range(fold):\n",
    "        # take one fold as training data\n",
    "        data_train_fold = train_data[i * fold_size:(i + 1) * fold_size]\n",
    "\n",
    "        ############################################# sgHCM classifier #################################################\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "        print(f\"Training data: {len(data_train_fold)}\")\n",
    "        classifier = sgHCM(k=3)\n",
    "        classifier.fit(data_train_fold)\n",
    "        predictions = [classifier(test_sample) for test_sample in test_data]\n",
    "        print(predictions)\n",
    "        \n",
    "        ################################# end #################################\n",
    "\n",
    "        \n",
    "    \n",
    "cross_validation(training_data, test_data, fold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
