{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 2\n",
    "\n",
    "Due Date 21 กันยายน 2566 ก่อนเวลา 23.00 น.\n",
    "\n",
    "ให้ส่งเป็น pdf file ผ่านระบบที่ mango.cmu.ac.th เท่านั้น\n",
    "\n",
    "Note ห้าม ใช้ library หรือ โปรแกรมสำหรับรูปใดๆ ทั้งสิ้น จะต้องเขียนโปรแกรมด้วยตัวเองทั้งหมด ห้าม ลอกงานคนอื่นถ้าลอกกันมา ทั้งคนให้ลอกและคนลอกจะได้รับคะแนนเป็น 0 ทั้งคู่ และให้แนบ program มาในส่วนของภาคผนวกของรายงานด้วย\n",
    "\n",
    "การบ้านนี้มี 2 ข้อ\n",
    "\n",
    "## 2.2 จงทำการทดลองโดยใช้ string grammar Hard C-means \n",
    "โดยใช้ data set ของ Copenhagen Chromosome Database ให้ใช้ train data set ในการ train (ชื่อ file จะเป็น difxda เช่น dif1da) และ test data set  (ชื่อ file จะเป็น difxdb เช่น dif1db) ในการ test ข้อมูลของ data set อยู่ที่ http://myweb.cmu.ac.th/sansanee.a/PatRecG/dataset/chrom.zip ทั้งนี้ data set มีจำนวน chromosome ทั้งหมด 2200 chromosome อย่าลืมอ่านคำแนะนำ data set ใน file CopenhagenChromosomeDataset.html ด้วย\n",
    "\n",
    " \n",
    "\n",
    "รายงานควรจะประกอบด้วย\n",
    "\n",
    "            1. รายละเอียดของทฤษฎีหรือวิธีการต่างๆที่ใช้\n",
    "\n",
    "            2. การออกแบบ algorithm เช่น pseudo-code, flowchart, ฯลฯ\n",
    "\n",
    "            3. ผลการทดลอง\n",
    "\n",
    "            4. การวิเคราะห์การทดลอง เช่น ได้ผลตามที่คาดไว้หรือไม่ มีสิ่งประหลาดเกิดขึ้นหรือไม่ บทสรุปที่ได้คืออะไร ฯลฯ\n",
    "\n",
    "            5. Well documented, structured, modular program listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files: 44\n",
      "# of training files: 22\n",
      "# of testing files:  22\n"
     ]
    }
   ],
   "source": [
    "# grab the name of files in the directory 'chrom'\n",
    "# excluding the file with 'html' extension\n",
    "# and store the names in a list\n",
    "\n",
    "chrom_files = glob.glob('chrom/*')\n",
    "chrom_files = [x for x in chrom_files if 'html' not in x]\n",
    "print(f'# of files: {len(chrom_files)}')\n",
    "\n",
    "# files ending with a are training data\n",
    "# files ending with b are testing data\n",
    "\n",
    "# list of training data\n",
    "train_files = [x for x in chrom_files if 'a' in x]\n",
    "print(f'# of training files: {len(train_files)}')\n",
    "\n",
    "# list of testing data\n",
    "test_files = [x for x in chrom_files if 'b' in x]\n",
    "print(f'# of testing files:  {len(test_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that reads the contents of a file\n",
    "# and returns a list of lines\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # for each line, take only the string after '\\t' and before '\\n'\n",
    "    lines = [x.split('\\t')[1].split('\\n')[0] for x in lines]\n",
    "\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data: 2200\n",
      "# of testing data: 2200\n"
     ]
    }
   ],
   "source": [
    "# read all the training data\n",
    "train_data = [read_file(x) for x in train_files]\n",
    "# flatten the list of lists\n",
    "train_data = [item for sublist in train_data for item in sublist]\n",
    "print(f'# of training data: {len(train_data)}')\n",
    "\n",
    "# read all the testing data\n",
    "test_data = [read_file(x) for x in test_files]\n",
    "# flatten the list of lists\n",
    "test_data = [item for sublist in test_data for item in sublist]\n",
    "print(f'# of testing data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String grammar hard C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Levenshtein distance function\n",
    "def levenshtein_distance(s1, s2):\n",
    "    # if one of the strings is empty, return the length of the other string\n",
    "    if len(s1) == 0:\n",
    "        return len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    # if the last characters of the strings are the same,\n",
    "    # return the distance between the strings without the last character\n",
    "    if s1[-1] == s2[-1]:\n",
    "        return levenshtein_distance(s1[:-1], s2[:-1])\n",
    "\n",
    "    # otherwise, return the minimum of the following:\n",
    "    # 1. distance between s1 and s2 without the last character of s1\n",
    "    # 2. distance between s1 and s2 without the last character of s2\n",
    "    # 3. distance between s1 and s2 without the last character of both s1 and s2\n",
    "    return 1 + min(levenshtein_distance(s1[:-1], s2),\n",
    "                   levenshtein_distance(s1, s2[:-1]),\n",
    "                   levenshtein_distance(s1[:-1], s2[:-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between 'kitten' and 'sitting' is 3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "distance = levenshtein_distance(string1, string2)\n",
    "print(f\"Levenshtein distance between '{string1}' and '{string2}' is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    # Create a matrix to store the distances between substrings of s1 and s2\n",
    "    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    for i in range(len(s1) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill in the matrix using dynamic programming\n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,  # Deletion\n",
    "                dp[i][j - 1] + 1,  # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    # The final value in the bottom-right corner of the matrix is the Levenshtein distance\n",
    "    return dp[len(s1)][len(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance between 'kitten' and 'sitting' is 3\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "string1 = \"kitten\"\n",
    "string2 = \"sitting\"\n",
    "distance = levenshtein_distance(string1, string2)\n",
    "print(f\"Levenshtein distance between '{string1}' and '{string2}' is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sgHCM:\n",
    "    # first, initial the class with k value\n",
    "    # then, call the class with test sample and train data\n",
    "    def __init__(self, k=3, max_iter=1000, tol=1e-5):\n",
    "        # number of clusters\n",
    "        self.k = k\n",
    "        # maximum iteration\n",
    "        self.max_iter = max_iter\n",
    "        # tolerance (epsilon)\n",
    "        self.tol = tol\n",
    "        # initialize the list of distances between old and new centroids very high\n",
    "        self.Et = np.inf\n",
    "        \n",
    "\n",
    "    def fit(self, train_data):\n",
    "        # randomly select k centroids from train_data (or prototype vectors)\n",
    "        self.centroids = np.random.choice(train_data, size=self.k, replace=False)\n",
    "        print(f'Initial centroids: {self.centroids}')\n",
    "\n",
    "        # initialize the list of clusters\n",
    "        self.clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # initialize the list of old centroids\n",
    "        self.old_centroids = None\n",
    "\n",
    "        # initialize the list of new centroids\n",
    "        self.new_centroids = None\n",
    "\n",
    "        # initialize the list of distances between test sample and each train data\n",
    "        self.distances = None\n",
    "\n",
    "        # initialize the list of predicted labels\n",
    "        self.predicted_label = None\n",
    "\n",
    "        for t in range(self.max_iter):\n",
    "            print(f'Iteration {t + 1}')\n",
    "            # assign each train data to the closest centroid\n",
    "            self.clusters = self.assign_clusters(train_data)\n",
    "\n",
    "            # update the centroids\n",
    "            self.old_centroids = self.centroids\n",
    "            self.new_centroids = self.update_centroids(train_data)\n",
    "\n",
    "            # compute the difference between old and new centroids\n",
    "            self.Et = self.compute_terminal_measure(self.old_centroids, self.new_centroids)\n",
    "\n",
    "            # if the difference is less than the tolerance, stop the iteration\n",
    "            if self.Et < self.tol:\n",
    "                break\n",
    "\n",
    "            # otherwise, update the centroids and continue\n",
    "            self.centroids = self.new_centroids\n",
    "\n",
    "        print(f'Final centroids: {self.centroids}')\n",
    "\n",
    "    def assign_clusters(self, train_data):\n",
    "        print(f'Assigning clusters...')\n",
    "        # initialize the list of clusters\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # for each train data\n",
    "        for i, train_sample in enumerate(train_data):\n",
    "            # compute the distance between test_sample and each train data\n",
    "            self.distances = self.levenshtein_distance(train_sample, train_data)\n",
    "\n",
    "            # find the index of the closest centroid\n",
    "            closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "            # assign the train sample to the closest centroid\n",
    "            clusters[closest_centroid].append(i)\n",
    "\n",
    "        print(f'Clusters: {clusters}')\n",
    "        return clusters\n",
    "    \n",
    "    def update_centroids(self, train_data):\n",
    "        # initialize the list of new centroids\n",
    "        new_centroids = []\n",
    "\n",
    "        # for each cluster\n",
    "        for cluster in self.clusters:\n",
    "            # compute the average of the train samples in the cluster\n",
    "            new_centroid = np.argmin()\n",
    "\n",
    "            # add the new centroid to the list\n",
    "            new_centroids.append(new_centroid)\n",
    "\n",
    "        print(f'New centroids: {new_centroids}')\n",
    "        return new_centroids\n",
    "\n",
    "\n",
    "    def __call__(self, test_sample):\n",
    "        # compute the distance between test_sample and each train data\n",
    "        self.distances = self.compute_distance(test_sample, train_data)\n",
    "\n",
    "        # find the index of the closest centroid\n",
    "        closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "        # assign the test sample to the closest centroid\n",
    "        self.predicted_label = closest_centroid\n",
    "\n",
    "        return self.predicted_label\n",
    "\n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        # Create a matrix to store the distances between substrings of s1 and s2\n",
    "        dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "        # Initialize the first row and column of the matrix\n",
    "        for i in range(len(s1) + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(len(s2) + 1):\n",
    "            dp[0][j] = j\n",
    "\n",
    "        # Fill in the matrix using dynamic programming\n",
    "        for i in range(1, len(s1) + 1):\n",
    "            for j in range(1, len(s2) + 1):\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,  # Deletion\n",
    "                    dp[i][j - 1] + 1,  # Insertion\n",
    "                    dp[i - 1][j - 1] + cost  # Substitution\n",
    "                )\n",
    "\n",
    "        # The final value in the bottom-right corner of the matrix is the Levenshtein distance\n",
    "        return dp[len(s1)][len(s2)]\n",
    "\n",
    "    def compute_distance(self, test_sample, train_data):\n",
    "        # compute the distance between test_sample and each train data\n",
    "        # return a list of distances\n",
    "        distances = []\n",
    "        for train_sample in train_data:\n",
    "            distance = self.levenshtein_distance(test_sample, train_sample)\n",
    "            distances.append(distance)\n",
    "        return distances\n",
    "    \n",
    "    def compute_terminal_measure(self, old, new):\n",
    "        # compute the difference between old and new centroids\n",
    "        Et = np.sum(np.abs(old - new))\n",
    "\n",
    "        # return the tolerance\n",
    "        return Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Initial centroids: ['A=Aa===E====e===Aa==a' 'A==A====a=A=a===E===d===A===b===A=a==a'\n",
      " 'A=A===a=====E==e========A==a==a']\n",
      "Iteration 1\n",
      "Assigning clusters...\n"
     ]
    }
   ],
   "source": [
    "# perform 10-fold cross validation on sgHCM classifier\n",
    "def cross_validation(train_data, test_data, fold=10):\n",
    "    # shuffle data before cross validation\n",
    "    np.random.shuffle(train_data)\n",
    "    fold_size = len(train_data) // fold\n",
    "\n",
    "    for i in range(fold):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        data_train_fold = np.concatenate([train_data[:start], train_data[end:]]) # train data\n",
    "\n",
    "        ############################################# sgHCM classifier #################################################\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "        classifier = sgHCM(k=3)\n",
    "        classifier.fit(data_train_fold)\n",
    "        predictions = [classifier(test_sample) for test_sample in test_data]\n",
    "        print(predictions)\n",
    "        \n",
    "        ################################# end #################################\n",
    "\n",
    "        \n",
    "    \n",
    "cross_validation(train_data, test_data, fold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
