{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 2 Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of files: 44\n",
      "# of training files: 22\n",
      "# of testing files:  22\n"
     ]
    }
   ],
   "source": [
    "# grab the name of files in the directory 'chrom'\n",
    "# excluding the file with 'html' extension\n",
    "# and store the names in a list\n",
    "\n",
    "chrom_files = glob.glob('chrom/*')\n",
    "chrom_files = [x for x in chrom_files if 'html' not in x]\n",
    "print(f'# of files: {len(chrom_files)}')\n",
    "\n",
    "# files ending with a are training data\n",
    "# files ending with b are testing data\n",
    "\n",
    "# list of training data\n",
    "train_files = [x for x in chrom_files if 'a' in x]\n",
    "print(f'# of training files: {len(train_files)}')\n",
    "\n",
    "# list of testing data\n",
    "test_files = [x for x in chrom_files if 'b' in x]\n",
    "print(f'# of testing files:  {len(test_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that reads the contents of a file\n",
    "# and returns a list of lines\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # for each line, take the string after '\\t' and before '\\n' as data\n",
    "    # the second number is label\n",
    "    data = [x.split('\\t')[1].split('\\n')[0] for x in lines]\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_label(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # for each line, take the string after '\\t' and before '\\n' as data\n",
    "    # the second number is label\n",
    "    labels = [x.split('\\t')[0] for x in lines]\n",
    "    # split labels into a list of lists\n",
    "    labels = [x.split(' ')[2] for x in labels]\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data: 2200\n",
      "# of training labels: 2200\n",
      "# of testing data: 2200\n",
      "# of testing labels: 2200\n"
     ]
    }
   ],
   "source": [
    "# read all the training data\n",
    "train_data = [read_data(x) for x in train_files]\n",
    "training_labels = [read_label(x) for x in train_files]\n",
    "# flatten the list of lists\n",
    "training_data = [item for sublist in train_data for item in sublist]\n",
    "training_labels = [item for sublist in training_labels for item in sublist]\n",
    "print(f'# of training data: {len(training_data)}')\n",
    "print(f'# of training labels: {len(training_labels)}')\n",
    "\n",
    "# read all the testing data\n",
    "test_data = [read_data(x) for x in test_files]\n",
    "# flatten the list of lists\n",
    "test_data = [item for sublist in test_data for item in sublist]\n",
    "test_labels = [read_label(x) for x in test_files]\n",
    "test_labels = [item for sublist in test_labels for item in sublist]\n",
    "print(f'# of testing data: {len(test_data)}')\n",
    "print(f'# of testing labels: {len(test_labels)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique training data: 2186\n",
      "# of unique training labels: 170\n",
      "# of unique testing data: 2189\n",
      "# of unique testing labels: 166\n"
     ]
    }
   ],
   "source": [
    "# number of unique training data\n",
    "unique_training_data = list(set(training_data))\n",
    "print(f'# of unique training data: {len(unique_training_data)}')\n",
    "\n",
    "# number of unique training labels\n",
    "unique_training_labels = list(set(training_labels))\n",
    "print(f'# of unique training labels: {len(unique_training_labels)}')\n",
    "\n",
    "# number of unique testing data\n",
    "unique_test_data = list(set(test_data))\n",
    "print(f'# of unique testing data: {len(unique_test_data)}')\n",
    "\n",
    "# number of unique testing labels\n",
    "unique_test_labels = list(set(test_labels))\n",
    "print(f'# of unique testing labels: {len(unique_test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique training data that appears more than once: 14\n",
      "number of times each unique training data appears: [2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "# of unique testing data that appears more than once: 11\n",
      "number of times each unique testing data appears: [2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# remove duplicates from training data\\n# remove duplicates from training labels\\n# remove duplicates from testing data\\n# remove duplicates from testing labels\\ntraining_data = list(set(training_data))\\ntraining_labels = list(set(training_labels))\\n\\ntest_data = list(set(test_data))\\ntest_labels = list(set(test_labels))\\n\\n# show the first training data and its label\\nprint(f'first training data: {training_data[0]}')\\nprint(f'first training label: {training_labels[0]}')\""
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the training data that appears more than once in the training data\n",
    "# and the number of times it appears\n",
    "# this is to check if there are any duplicates in the training data\n",
    "# if there are duplicates, we need to remove them\n",
    "# if there are no duplicates, we can move on to the next step\n",
    "unique_training_data, counts = np.unique(training_data, return_counts=True)\n",
    "print(f'# of unique training data that appears more than once: {len(unique_training_data[counts > 1])}')\n",
    "# print(f'unique training data that appears more than once: {unique_training_data[counts > 1]}')\n",
    "print(f'number of times each unique training data appears: {counts[counts > 1]}')\n",
    "\n",
    "# show the testing data that appears more than once in the testing data\n",
    "# and the number of times it appears\n",
    "# this is to check if there are any duplicates in the testing data\n",
    "# if there are duplicates, we need to remove them\n",
    "# if there are no duplicates, we can move on to the next step\n",
    "unique_test_data, counts = np.unique(test_data, return_counts=True)\n",
    "print(f'# of unique testing data that appears more than once: {len(unique_test_data[counts > 1])}')\n",
    "# print(f'unique testing data that appears more than once: {unique_test_data[counts > 1]}')\n",
    "print(f'number of times each unique testing data appears: {counts[counts > 1]}')\n",
    "\n",
    "'''# remove duplicates from training data\n",
    "# remove duplicates from training labels\n",
    "# remove duplicates from testing data\n",
    "# remove duplicates from testing labels\n",
    "training_data = list(set(training_data))\n",
    "training_labels = list(set(training_labels))\n",
    "\n",
    "test_data = list(set(test_data))\n",
    "test_labels = list(set(test_labels))\n",
    "\n",
    "# show the first training data and its label\n",
    "print(f'first training data: {training_data[0]}')\n",
    "print(f'first training label: {training_labels[0]}')'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data that is duplicated in testing data: 18\n",
      "I amm going to ignore the duplicates for now as it takes a lot of time to remove them while keeping the labels in order.\n"
     ]
    }
   ],
   "source": [
    "# show the training data that is duplicated in testing data\n",
    "print(f'# of training data that is duplicated in testing data: {len(set(training_data).intersection(test_data))}')\n",
    "\n",
    "'''# remove the training data that is duplicated in testing data\n",
    "# remove the training labels that is duplicated in testing labels\n",
    "training_data = list(set(training_data) - set(test_data))\n",
    "training_labels = list(set(training_labels) - set(test_labels))\n",
    "'''\n",
    "print('I amm going to ignore the duplicates for now as it takes a lot of time to remove them while keeping the labels in order.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String grammar hard C-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data in the dictionary: 2186\n",
      "# of data in the dictionary: 4357\n",
      "test the label_lookup class\n",
      "data: A=====B=a==B===a==A==a=Aa===A===a===C==a=A=a==A==c=====Aa=A==c==A=a===a\n",
      "label: 52\n",
      "label from class: 52\n"
     ]
    }
   ],
   "source": [
    "# class that returns the label of the given data\n",
    "class label_lookup:\n",
    "    def __init__(self, data, labels):\n",
    "        # create a dictionary with data as key and label as value\n",
    "        self.lookup = dict(zip(data, labels))\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # return the label of the given data\n",
    "        return self.lookup[data]\n",
    "    \n",
    "    def add_list(self, data, labels):\n",
    "        # add a list of data and labels to the dictionary\n",
    "        self.lookup.update(dict(zip(data, labels)))\n",
    "\n",
    "    # return list of labels of the given list of data\n",
    "    def get_labels(self, data):\n",
    "        try:\n",
    "            return [self.lookup[x] for x in data]\n",
    "        except KeyError:\n",
    "            return self.lookup[data]\n",
    "    \n",
    "    # return number of data in the dictionary\n",
    "    def __len__(self):\n",
    "        return len(self.lookup)\n",
    "    \n",
    "    # return all the keys of  a given value\n",
    "    def get_key(self, value):\n",
    "        return [k for k,v in self.lookup.items() if v == value]\n",
    "    \n",
    "\n",
    "# create an instance of label_lookup\n",
    "lookup = label_lookup(training_data, training_labels)\n",
    "print(f'# of data in the dictionary: {len(lookup)}')\n",
    "lookup.add_list(test_data, test_labels)\n",
    "print(f'# of data in the dictionary: {len(lookup)}')\n",
    "\n",
    "print('test the label_lookup class')\n",
    "print(f'data: {training_data[0]}')\n",
    "print(f'label: {training_labels[0]}')\n",
    "print(f'label from class: {lookup(training_data[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class sgHCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sgHCM:\n",
    "    # first, initial the class with k value\n",
    "    # then, call the class with test sample and train data\n",
    "    def __init__(self, k=3, max_iter=10, tol=1e-5):\n",
    "        # number of clusters\n",
    "        self.k = k\n",
    "        # maximum iteration\n",
    "        self.max_iter = max_iter\n",
    "        # tolerance (epsilon)\n",
    "        self.tol = tol\n",
    "        # initialize the list of distances between old and new centroids very high\n",
    "        self.Et = np.inf\n",
    "        self.train_data = None\n",
    "        self.centroids = None\n",
    "\n",
    "    def __call__(self, test_sample):\n",
    "        # compute the distance between test_sample and the centroids\n",
    "        self.distances = self.compute_distance(test_sample, self.centroids)\n",
    "\n",
    "        # find the index of the closest centroid\n",
    "        closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "        # label of the closest centroid\n",
    "        self.predicted_label = lookup(self.centroids[closest_centroid])\n",
    "\n",
    "        return self.centroids[closest_centroid], self.predicted_label\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        # randomly select k centroids from train_data (or prototype vectors)\n",
    "        self.centroids = np.random.choice(train_data, size=self.k, replace=False)\n",
    "\n",
    "        # initialize the list of clusters\n",
    "        self.clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # initialize the list of old centroids\n",
    "        self.old_centroids = None\n",
    "\n",
    "        # initialize the list of new centroids\n",
    "        self.new_centroids = None\n",
    "\n",
    "        # initialize the list of distances between test sample and each train data\n",
    "        self.distances = None\n",
    "\n",
    "        # initialize the list of predicted labels\n",
    "        self.predicted_label = None\n",
    "\n",
    "        self.u_ik = None\n",
    "        self.n_it = None\n",
    "\n",
    "        for t in range(self.max_iter):\n",
    "            print(f'\\nIteration {t + 1}')\n",
    "            # assign each train data to the closest centroid\n",
    "            self.clusters, self.u_ik = self.assign_clusters(self.train_data)\n",
    "\n",
    "            # n_it is the number of train samples in the i-th cluster\n",
    "            self.n_it = np.sum(self.u_ik, axis=0)\n",
    "\n",
    "            # update the centroids\n",
    "            self.old_centroids = self.centroids\n",
    "            self.new_centroids = self.update_centroids(self.train_data)\n",
    "            \n",
    "            _ = self.centroids_uniqueness(self.new_centroids)\n",
    "            # print(f'New centroids: {self.new_centroids}')\n",
    "\n",
    "            # compute the difference between old and new centroids\n",
    "            self.Et = self.compute_terminal_measure(self.old_centroids, self.new_centroids)\n",
    "\n",
    "            # if the difference is less than the tolerance, stop the iteration\n",
    "            if self.Et < self.tol:\n",
    "                break\n",
    "\n",
    "            # otherwise, update the centroids and continue\n",
    "            self.centroids = self.new_centroids\n",
    "\n",
    "        print(f'Final centroids: {self.centroids}')\n",
    "        # return the list of clusters\n",
    "        return self.clusters\n",
    "\n",
    "    def assign_clusters(self, train_data):\n",
    "        # compute the distance between the train data and each centroid\n",
    "        # initialize the list of clusters\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "\n",
    "        # for each train data\n",
    "        for i, train_sample in enumerate(train_data):\n",
    "            # compute the distance between test_sample and the centroid\n",
    "            self.distances = self.compute_distance(train_sample, self.centroids)\n",
    "\n",
    "            # find the index of the closest centroid\n",
    "            closest_centroid = np.argmin(self.distances)\n",
    "\n",
    "            # assign the train sample to the closest centroid\n",
    "            clusters[closest_centroid].append(i)\n",
    "\n",
    "        # create the u_ik matrix, which is a matrix of 0s and 1s\n",
    "        # u_ik[i][j] = 1 if the i-th train sample belongs to the j-th cluster\n",
    "        # u_ik[i][j] = 0 otherwise\n",
    "        u_ik = np.zeros((len(train_data), self.k))\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            for j in cluster:\n",
    "                u_ik[j][i] = 1\n",
    "\n",
    "        self.n_it = np.sum(u_ik, axis=0)\n",
    "\n",
    "        return clusters, u_ik\n",
    "    \n",
    "    def update_centroids(self, train_data):\n",
    "        # compute the distance between each sample and other samples in the same cluster\n",
    "        # the sample with the minimum sum of the distances to other samples in the same cluster is the new centroid for that cluster  \n",
    "\n",
    "        # initialize the list of new centroids\n",
    "        new_centroids = []\n",
    "\n",
    "        # for each cluster\n",
    "        for i in range(len(self.clusters)):\n",
    "            # c_ij is a list of the sum of the distances to other samples in the same cluster\n",
    "            c_ij = []\n",
    "\n",
    "            for j in range(len(self.clusters[i])):\n",
    "                # compute the distance between the sample and other samples in the same cluster excluding itself\n",
    "                # list of samples excluding the sample itself\n",
    "                samples = [x for x in self.clusters[i] if x != j]\n",
    "\n",
    "                # take train_data at the index of samples\n",
    "                train = np.array(train_data.copy())\n",
    "                other_samples = train[samples]\n",
    "\n",
    "                distances = self.compute_distance(train_data[j], other_samples)\n",
    "                \n",
    "                c_ij.append(np.sum(distances/self.n_it[i], axis=0))\n",
    "\n",
    "            # find the index of the sample with the minimum sum \n",
    "            # of the distances to other samples in the same cluster\n",
    "            try:\n",
    "                alpha_q = np.argmin(c_ij)\n",
    "            # except there is an error, which is the case when c_ij is empty, redraw the centroids\n",
    "            except:\n",
    "                print(f'Found empty cluster')\n",
    "                self.centroids = np.random.choice(self.train_data, size=self.k, replace=False)\n",
    "                return self.centroids\n",
    "\n",
    "            # add the new centroid to the list\n",
    "            new_centroids.append(train_data[alpha_q])\n",
    "\n",
    "        return new_centroids\n",
    "    \n",
    "    def centroids_uniqueness(self, centroids):\n",
    "        # check if the new centroids are unique\n",
    "        # return True if the new centroids are unique\n",
    "        # return False if the new centroids are not unique\n",
    "        # print(f'Checking centroids uniqueness...')\n",
    "        for i in range(len(centroids)):\n",
    "            for j in range(len(centroids)):\n",
    "                if i != j:\n",
    "                    if centroids[i] == centroids[j]:\n",
    "                        print(f'Found centroids not unique')\n",
    "                        # redraw the centroids\n",
    "                        self.centroids = np.random.choice(self.train_data, size=self.k, replace=False)\n",
    "                        return False\n",
    "        return True\n",
    "    \n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        # Create a matrix to store the distances between substrings of s1 and s2\n",
    "        distance_matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "        # Initialize the first row and column of the matrix\n",
    "        for i in range(len(s1) + 1):\n",
    "            distance_matrix[i][0] = i\n",
    "        for j in range(len(s2) + 1):\n",
    "            distance_matrix[0][j] = j\n",
    "\n",
    "        # Fill in the matrix using dynamic programming\n",
    "        for i in range(1, len(s1) + 1):\n",
    "            for j in range(1, len(s2) + 1):\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "                distance_matrix[i][j] = min(\n",
    "                    distance_matrix[i - 1][j] + 1,  # Deletion\n",
    "                    distance_matrix[i][j - 1] + 1,  # Insertion\n",
    "                    distance_matrix[i - 1][j - 1] + cost  # Substitution\n",
    "                )\n",
    "\n",
    "        # The final value in the bottom-right corner of the matrix is the Levenshtein distance\n",
    "        return distance_matrix[len(s1)][len(s2)]\n",
    "\n",
    "    def compute_distance(self, test_sample, train_data):\n",
    "        # compute the distance between test_sample and each train data\n",
    "        # return a list of distances\n",
    "        distances = []\n",
    "        for train_sample in train_data:\n",
    "            distance = self.levenshtein_distance(test_sample, train_sample)\n",
    "            distances.append(distance)\n",
    "        return distances\n",
    "    \n",
    "    def compute_terminal_measure(self, old_V, new_V):\n",
    "        # compute the levenstein distance between old and new centroids\n",
    "        # return the sum of the levenstein distances\n",
    "        Et = []\n",
    "        for i in range(len(old_V)):\n",
    "            Et.append(self.levenshtein_distance(old_V[i], new_V[i]))\n",
    "    \n",
    "        Et = np.array(Et)\n",
    "        Et = np.sum(Et)\n",
    "        print(f'Terminal measure: Et = {Et}')\n",
    "\n",
    "        # return the tolerance\n",
    "        return Et\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 10-fold cross validation on sgHCM classifier\n",
    "def cross_validation(train_data, train_labels, test_data, k_num = 2, fold=10):\n",
    "    # shuffle the number of samples in the training data \n",
    "    # and rearrange the labels accordingly\n",
    "\n",
    "    # combine the training data and labels\n",
    "    train_data = np.array(train_data)\n",
    "    train_data = np.c_[train_data, train_labels]\n",
    "\n",
    "    # shuffle the training data\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    # split the training data and labels\n",
    "    train_labels = train_data[:, -1]\n",
    "    train_data = train_data[:, :-1]    \n",
    "    # convert train_data to 1 dimensional array\n",
    "    train_data = train_data.ravel()\n",
    "\n",
    "    fold_size = len(train_data) // fold\n",
    "\n",
    "    accuracy_list = []\n",
    "    num_of_unique_labels = []\n",
    "\n",
    "    for i in range(fold):\n",
    "        ############################ sgHCM classifier ##########################\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "        \n",
    "        # take one fold as training data\n",
    "        data_train_fold = train_data[i * fold_size: (i + 1) * fold_size]\n",
    "        data_test_fold = test_data[i * fold_size: (i + 1) * fold_size]\n",
    "\n",
    "        # show number of unique labels in the training data\n",
    "        print(f'\\nNumber of unique labels in the training data: {len(np.unique(train_labels))}')\n",
    "        num_of_unique_labels.append(len(np.unique(train_labels)))\n",
    "        \n",
    "        classifier = sgHCM(k=k_num)\n",
    "        classifier.fit(data_train_fold)\n",
    "\n",
    "        # print('\\nPredicting the test dataset...')\n",
    "        print(f'Predictions: (test sample, test label, predicted centroid label, predicted centroid)')\n",
    "        predictions = []\n",
    "        accuracy = 0\n",
    "        \n",
    "        for j, test_sample in enumerate(data_test_fold):\n",
    "            predicted_centroid, predicted_label = classifier(test_sample)\n",
    "            predictions.append([test_sample, predicted_label, predicted_centroid])\n",
    "            if j < 5:\n",
    "                print(f'{test_sample}, {lookup(test_sample)}, {predicted_label}, {predicted_centroid}')      \n",
    "            if predicted_label == lookup(test_sample):\n",
    "                accuracy += 1\n",
    "\n",
    "        accuracy = accuracy / len(data_test_fold)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "        print('\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "\n",
    "    print(f'\\nAverage accuracy: {np.mean(accuracy_list)}')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    return np.mean(accuracy_list), np.mean(num_of_unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "k = 2\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Number of unique labels in the training data: 170\n",
      "\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal measure: Et = 36\n",
      "\n",
      "Iteration 2\n",
      "Terminal measure: Et = 0\n",
      "Final centroids: ['A====B======a===D====d======A==b==a', 'A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a']\n",
      "Predictions: (test sample, test label, predicted centroid label, predicted centroid)\n",
      "A=B==a=A=a===B=a=A=aA=a=Aa======B====a==B===d====A==b====a, 52, 140, A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a\n",
      "A==B==a=A=a==A==a=A=a==A=a=====C===c====D==e===A=a==A=a=a, 6, 140, A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a\n",
      "A=A=a=B==a==B=a=A=a=A==a=A=a==B====b===C===d=====A==b===a, 40, 140, A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a\n",
      "A=Aa==B==b===A====B====a=A=a==B=====c====D===e=====A==a=a, 33, 140, A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a\n",
      "A==B=====a==A===a====B==a==B===aA===a===B===d==A=aA==b==a, 136, 140, A=====E==c===A==a==A=a=A===c=====E===e====E====d===A===b===Aa=a\n",
      "Accuracy: 0.0\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Number of unique labels in the training data: 170\n",
      "\n",
      "Iteration 1\n",
      "Terminal measure: Et = 32\n",
      "\n",
      "Iteration 2\n",
      "Terminal measure: Et = 10\n",
      "\n",
      "Iteration 3\n",
      "Terminal measure: Et = 0\n",
      "Final centroids: ['A=A==a=====E==e===A=a==A==a=a', 'A======B===a===A==b========E====d============a=a']\n",
      "Predictions: (test sample, test label, predicted centroid label, predicted centroid)\n",
      "A==A=a===B=aA===b==B===b==========E===e======A===a=a, 53, 34, A======B===a===A==b========E====d============a=a\n",
      "A==Aa====C====b=====A==b=========E=====e=====A==a==a, 7, 34, A======B===a===A==b========E====d============a=a\n",
      "A====B=a==C=====c==B====c=======E=======e====A===a=a, 33, 34, A======B===a===A==b========E====d============a=a\n",
      "A=A=a===C=a=B==b===A==c=========E======e=====A===a=a, 70, 34, A======B===a===A==b========E====d============a=a\n",
      "A=A==a===E===c==Aa=B===d=======C==a=A===c===A====a=a, 125, 34, A======B===a===A==b========E====d============a=a\n",
      "Accuracy: 0.00909090909090909\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Number of unique labels in the training data: 170\n",
      "\n",
      "Iteration 1\n",
      "Terminal measure: Et = 28\n",
      "\n",
      "Iteration 2\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# k_list = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]\n",
    "k_list = [2, 6, 10, 14, 18, 22]\n",
    "\n",
    "for k in k_list:\n",
    "    print(f'\\n\\nk = {k}')\n",
    "    accuracy, num_of_unique_labels = cross_validation(training_data, training_labels, test_data, k_num = k)\n",
    "    results.append([k, accuracy, num_of_unique_labels])\n",
    "    # cross_validation(training_data, training_labels, test_data, k_num = k)\n",
    "print('\\n\\nDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n\\nResults: (k, accuracy, number of unique labels)')\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
