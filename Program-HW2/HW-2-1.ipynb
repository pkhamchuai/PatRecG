{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 2\n",
    "\n",
    "Due Date 21 กันยายน 2566 ก่อนเวลา 23.00 น.\n",
    "\n",
    "ให้ส่งเป็น pdf file ผ่านระบบที่ mango.cmu.ac.th เท่านั้น\n",
    "\n",
    "Note ห้าม ใช้ library หรือ โปรแกรมสำหรับรูปใดๆ ทั้งสิ้น จะต้องเขียนโปรแกรมด้วยตัวเองทั้งหมด ห้าม ลอกงานคนอื่นถ้าลอกกันมา ทั้งคนให้ลอกและคนลอกจะได้รับคะแนนเป็น 0 ทั้งคู่ และให้แนบ program มาในส่วนของภาคผนวกของรายงานด้วย\n",
    "\n",
    "การบ้านนี้มี 2 ข้อ\n",
    "\n",
    "## 2.1 จงทำการทดลองโดยใช้ K-nn และอย่าลืมทำการประเมินประสิทธิภาพของวิธีการนี้ \n",
    "เช่นใช้ 10% cross validation หรือ เปรียบเทียบกับ Bayes (Maximum Likelihood) และอื่นๆ ให้ใช้ dataset ใดก็ได้เช่น TWOCLASS, IRIS (Iris และ TWOCLASS มี format ที่เหมือนกันนั่นคือ f1 f2 f3 f4 class-label), CROSS, ELLIPSE (Cross และ Ellipse มี format ที่เหมือนกันนั่นคือ\n",
    "\n",
    "                                               p#\n",
    "\n",
    "                                               f1 f2\n",
    "\n",
    "                                               0 1        %หมายถึง class 2 ถ้าเป็น 1 0 (class 1))\n",
    "\n",
    "หรือ dataset อื่นๆ ถ้ามี\n",
    "\n",
    "รายงานควรจะประกอบด้วย\n",
    "\n",
    "            1. รายละเอียดของทฤษฎีหรือวิธีการต่างๆที่ใช้\n",
    "\n",
    "            2. การออกแบบ algorithm เช่น pseudo-code, flowchart, ฯลฯ\n",
    "\n",
    "            3. ผลการทดลอง\n",
    "\n",
    "            4. การวิเคราะห์การทดลอง เช่น ได้ผลตามที่คาดไว้หรือไม่ มีสิ่งประหลาดเกิดขึ้นหรือไม่ บทสรุปที่ได้คืออะไร ฯลฯ\n",
    "\n",
    "            5. Well documented, structured, modular program listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"TWOCLASS.dat\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "        data = content.split()\n",
    "        # print(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found!\")\n",
    "\n",
    "# drop the first 6 elements from list 'data'\n",
    "data = data[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  200\n"
     ]
    }
   ],
   "source": [
    "data_processed = []\n",
    "\n",
    "for i in range(0, len(data), 5):\n",
    "    data_processed.append([float(data[i]), float(data[i+1]), float(data[i+2]), \\\n",
    "                           float(data[i+3]), int(data[i+4])])\n",
    "\n",
    "# print length of data\n",
    "print(\"Length of data: \", len(data_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Separate data by class\\nclass1_data = [sample[:-1] for sample in data_processed if sample[-1] == 1]\\nclass2_data = [sample[:-1] for sample in data_processed if sample[-1] == 2]\\n\\n# Features (replace these labels with your actual feature names)\\nfeature_labels = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\\n\\n# Create a subplot of 2x2 graphs, each with a size of 10x10\\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\\n\\n# Plot histograms for each feature\\nfor i in range(len(feature_labels)):\\n    ax = axs[i // 2, i % 2]  # Get the appropriate subplot\\n    ax.hist([x[i] for x in class1_data], bins=10, alpha=0.5, color='blue', label='Class 1', edgecolor='black')\\n    ax.hist([x[i] for x in class2_data], bins=10, alpha=0.5, color='red', label='Class 2', edgecolor='black')\\n    ax.set_xlabel(feature_labels[i])\\n    ax.set_ylabel('Frequency')\\n    ax.set_title(f'Histogram of {feature_labels[i]} by Class')\\n    ax.legend()\\n    # ax.grid(True)\\n\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Separate data by class\n",
    "class1_data = [sample[:-1] for sample in data_processed if sample[-1] == 1]\n",
    "class2_data = [sample[:-1] for sample in data_processed if sample[-1] == 2]\n",
    "\n",
    "# Features (replace these labels with your actual feature names)\n",
    "feature_labels = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\n",
    "\n",
    "# Create a subplot of 2x2 graphs, each with a size of 10x10\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Plot histograms for each feature\n",
    "for i in range(len(feature_labels)):\n",
    "    ax = axs[i // 2, i % 2]  # Get the appropriate subplot\n",
    "    ax.hist([x[i] for x in class1_data], bins=10, alpha=0.5, color='blue', label='Class 1', edgecolor='black')\n",
    "    ax.hist([x[i] for x in class2_data], bins=10, alpha=0.5, color='red', label='Class 2', edgecolor='black')\n",
    "    ax.set_xlabel(feature_labels[i])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Histogram of {feature_labels[i]} by Class')\n",
    "    ax.legend()\n",
    "    # ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood \n",
    "From previous homework, used for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Estimate Parameters (Mean vectors and Covariance matrices)\n",
    "def estimate_parameters(data):\n",
    "    num_features = len(data[0]) - 1  # Exclude the last column (class label)\n",
    "    num_classes = int(max(data, key=lambda x: x[-1])[-1])  # Assuming class labels are 1-indexed\n",
    "\n",
    "    mean_vectors = {i: np.zeros(num_features) for i in range(1, num_classes + 1)}\n",
    "    covariance_matrices = {i: np.zeros((num_features, num_features)) for i in range(1, num_classes + 1)}\n",
    "    class_counts = {i: 0 for i in range(1, num_classes + 1)}\n",
    "\n",
    "    # Calculate the sum of feature values for each class\n",
    "    for row in data:\n",
    "        class_label = int(row[-1])\n",
    "        class_counts[class_label] += 1\n",
    "        for i in range(num_features):\n",
    "            mean_vectors[class_label][i] += row[i]\n",
    "\n",
    "    # Calculate the mean vectors\n",
    "    for class_label in mean_vectors:\n",
    "        mean_vectors[class_label] /= class_counts[class_label]\n",
    "\n",
    "    # Calculate the covariance matrices\n",
    "    for row in data:\n",
    "        class_label = int(row[-1])\n",
    "        x_minus_mean = row[:-1] - mean_vectors[class_label]\n",
    "        x_minus_mean = x_minus_mean.reshape((-1, 1))  # Convert to column vector\n",
    "        covariance_matrices[class_label] += np.dot(x_minus_mean, x_minus_mean.T)\n",
    "\n",
    "    for class_label in covariance_matrices:\n",
    "        covariance_matrices[class_label] /= (class_counts[class_label] - 1)\n",
    "\n",
    "    return mean_vectors, covariance_matrices\n",
    "\n",
    "# Step 2: Minimum Risk Bayes Decision Theoretic Classifier\n",
    "def multivariate_normal_pdf(x, mean, covariance_matrix):\n",
    "    # Calculate the multivariate normal probability density function (PDF) for a given test sample 'x'\n",
    "    # with the given mean and covariance matrix.\n",
    "    k = len(x)\n",
    "    coefficient = 1.0 / ((2 * np.pi) ** (k / 2) * np.linalg.det(covariance_matrix))\n",
    "\n",
    "    # Calculate (x - mean)\n",
    "    x_minus_mean = x - mean\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    inv_covariance = np.linalg.inv(covariance_matrix)\n",
    "\n",
    "    # Calculate the Mahalanobis distance squared\n",
    "    mahalanobis_dist_sq = np.dot(x_minus_mean, np.dot(inv_covariance, x_minus_mean))\n",
    "\n",
    "    # Calculate the exponent\n",
    "    exponent = -0.5 * mahalanobis_dist_sq\n",
    "\n",
    "    return coefficient * np.exp(exponent)\n",
    "\n",
    "def minimum_risk_classifier(test_sample, mean_vectors, covariance_matrices, prior_probabilities):\n",
    "    num_classes = len(mean_vectors)\n",
    "    risks = [0] * num_classes\n",
    "\n",
    "    for class_label in range(1, num_classes + 1):\n",
    "        mean_vector = np.array(mean_vectors[class_label])\n",
    "        covariance_matrix = np.array(covariance_matrices[class_label])\n",
    "        \n",
    "        # Calculate the multivariate normal PDF for the current class\n",
    "        pdf = multivariate_normal_pdf(test_sample, mean_vector, covariance_matrix)\n",
    "        \n",
    "        # Calculate the risk for the current class, which is the negative log-PDF plus the log-prior probability.\n",
    "        risks[class_label - 1] = -np.log(pdf) + np.log(prior_probabilities[class_label])\n",
    "\n",
    "    # Choose the class with the minimum risk as the predicted class label.\n",
    "    predicted_label = np.argmin(risks) + 1\n",
    "    return predicted_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    # k-Nearest Neighbors Classifier\n",
    "    # first, initial the class with k value\n",
    "    # then, call the class with test sample and train data\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, test_sample, train_data):\n",
    "        k_nearest_neighbors = self.compute_k_nearest_neighbors(test_sample, train_data)\n",
    "        class_labels = [sample[-1] for sample in k_nearest_neighbors]\n",
    "        predicted_label = max(set(class_labels), key=class_labels.count)\n",
    "        return predicted_label\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def compute_k_nearest_neighbors(self, test_sample, train_data):\n",
    "        distances = []\n",
    "\n",
    "        for train_sample_features in train_data:\n",
    "            test_sample_ = test_sample[:-1]\n",
    "            train_sample_features_ = train_sample_features[:-1]\n",
    "            distance = self.euclidean_distance(test_sample_, train_sample_features_)\n",
    "            distances.append((train_sample_features, distance))\n",
    "\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        k_nearest_neighbors = [sample[0] for sample in distances[:self.k]]\n",
    "        return k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 0.95\n",
      "\n",
      "Fold 2\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "Fold 3\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "Fold 4\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "Fold 5\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "Fold 6\n",
      "k-NN accuracy: 0.95\n",
      "MLC accuracy: 0.95\n",
      "\n",
      "Fold 7\n",
      "k-NN accuracy: 0.90\n",
      "MLC accuracy: 0.90\n",
      "\n",
      "Fold 8\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "Fold 9\n",
      "k-NN accuracy: 0.90\n",
      "MLC accuracy: 0.95\n",
      "\n",
      "Fold 10\n",
      "k-NN accuracy: 1.00\n",
      "MLC accuracy: 1.00\n",
      "\n",
      "k-NN average accuracy: 0.97\n",
      "MLC average accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# perform 10-fold cross validation on maximum likelihood classifier and KNN classifier\n",
    "def cross_validation(data, k=10):\n",
    "    # shuffle data before cross validation\n",
    "    np.random.shuffle(data)\n",
    "    fold_size = len(data) // k\n",
    "    accuracy_scores_knn = []\n",
    "    accuracy_scores_mlc = []\n",
    "\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        data_test_fold = data[start:end]\n",
    "        data_train_fold = np.concatenate([data[:start], data[end:]])\n",
    "        y_test_fold = data_test_fold[:, -1]\n",
    "\n",
    "        ############################################# k-NN classifier ####################################################\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "        knn = KNNClassifier(k=3)\n",
    "        y_pred = [knn(x, data_train_fold) for x in data_test_fold]\n",
    "        accuracy = np.sum(y_pred == y_test_fold) / len(y_test_fold)\n",
    "        accuracy_scores_knn.append(accuracy)\n",
    "        print(f\"k-NN accuracy: {accuracy:.2f}\")\n",
    "\n",
    "        ###################################### Maximum likelihood classifier #############################################\n",
    "        # calculate prior probabilities from data_train_fold\n",
    "        y_train_fold = data_train_fold[:, -1]\n",
    "        prior_probabilities = {i: np.sum(y_train_fold == i) / len(y_train_fold) for i in np.unique(y_train_fold)}\n",
    "        # print(f\"Prior probabilities: {prior_probabilities}\")\n",
    "        mean_vectors, covariance_matrices = estimate_parameters(data_train_fold)\n",
    "\n",
    "        mlc_predicted_labels = []  # List to store the predicted labels for the current fold\n",
    "        for sample in data_test_fold:\n",
    "            test_sample = sample[:-1]\n",
    "            true_label = int(sample[-1])\n",
    "            predicted_label = minimum_risk_classifier(test_sample, mean_vectors, covariance_matrices, prior_probabilities)\n",
    "            mlc_predicted_labels.append(predicted_label)  # Store the predicted label for the current sample\n",
    "\n",
    "        mlc_accuracy = 1 - (np.sum(mlc_predicted_labels != y_test_fold) / len(y_test_fold))\n",
    "\n",
    "        # Display results for the current fold\n",
    "        print(f\"MLC accuracy: {mlc_accuracy:.2f}\")\n",
    "        accuracy_scores_mlc.append(mlc_accuracy)\n",
    "        ################################# end of Maximum likelihood classifier ###########################################\n",
    "\n",
    "    avg_accuracy_knn = np.mean(accuracy_scores_knn)\n",
    "    avg_accuracy_mlc = np.mean(accuracy_scores_mlc)\n",
    "    print(f\"\\nk-NN average accuracy: {avg_accuracy_knn:.2f}\")\n",
    "    print(f\"MLC average accuracy: {avg_accuracy_mlc:.2f}\")\n",
    "    # return avg_accuracy_knn, avg_accuracy_mlc\n",
    "    \n",
    "cross_validation(np.array(data_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
